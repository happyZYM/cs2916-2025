@online{li_LIMRLessMore_2025,
  title = {{{LIMR}}: {{Less}} Is {{More}} for {{RL Scaling}}},
  shorttitle = {{{LIMR}}},
  author = {Li, Xuefeng and Zou, Haoyang and Liu, Pengfei},
  date = {2025-02-17},
  eprint = {2502.11886},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.11886},
  url = {http://arxiv.org/abs/2502.11886},
  urldate = {2025-04-10},
  abstract = {In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7\% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0\% and 22.2\% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/zym/Zotero/storage/WU6AVSL7/Li 等 - 2025 - LIMR Less is More for RL Scaling.pdf;/home/zym/Zotero/storage/Q4ZDHX8M/2502.html}
}

@online{yu_DAPOOpenSourceLLM_2025,
  title = {{{DAPO}}: {{An Open-Source LLM Reinforcement Learning System}} at {{Scale}}},
  shorttitle = {{{DAPO}}},
  author = {Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and Liu, Xin and Lin, Haibin and Lin, Zhiqi and Ma, Bole and Sheng, Guangming and Tong, Yuxuan and Zhang, Chi and Zhang, Mofan and Zhang, Wang and Zhu, Hang and Zhu, Jinhua and Chen, Jiaze and Chen, Jiangjie and Wang, Chengyi and Yu, Hongli and Dai, Weinan and Song, Yuxuan and Wei, Xiangpeng and Zhou, Hao and Liu, Jingjing and Ma, Wei-Ying and Zhang, Ya-Qin and Yan, Lin and Qiao, Mu and Wu, Yonghui and Wang, Mingxuan},
  date = {2025-03-18},
  eprint = {2503.14476},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.14476},
  url = {http://arxiv.org/abs/2503.14476},
  urldate = {2025-04-05},
  abstract = {Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the \$\textbackslash textbf\{D\}\$ecoupled Clip and \$\textbackslash textbf\{D\}\$ynamic s\$\textbackslash textbf\{A\}\$mpling \$\textbackslash textbf\{P\}\$olicy \$\textbackslash textbf\{O\}\$ptimization (\$\textbackslash textbf\{DAPO\}\$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,hok1v1},
  file = {/home/zym/Zotero/storage/CW8TEP54/Yu 等 - 2025 - DAPO An Open-Source LLM Reinforcement Learning System at Scale.pdf;/home/zym/Zotero/storage/F8PVVMCK/2503.html}
}
